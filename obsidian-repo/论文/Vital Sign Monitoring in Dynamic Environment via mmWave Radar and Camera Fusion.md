
# 1 摘要

利用更少的信号来识别人体生命体征（如呼吸和心跳）是健康和安全领域一个具有吸引力的解决方案。然而，受试者的身体运动和实际环境的变化会导致心跳和呼吸频率估计不准确。本文提出了一种基于毫米波雷达和相机融合的生命体征监测系统，该系统在动态场景中（*如有人在被跟踪的受试者周围走动，或受试者挥动手臂和原地踏步时*）能够表现出一致的良好性能。系统中开发了三个主要处理模块，以实现稳健的感知。首先，我们利用相机辅助毫米波雷达准确定位感兴趣的受试者。其次，我们利用计算出的受试者位置形成发射和接收波束成形器，这可以提高目标反射功率并减弱动态干扰的影响。第三，我们提出了一种加权多通道变分模态分解（WMC-VMD）算法，以分离由于受试者身体运动引起的弱生命体征信号。实验结果表明，呼吸速率（RR）和心跳速率（HR）的90%分位数误差分别小于0.5 RPM（每分钟呼吸次数）和6 BPM（每分钟心跳次数）。

- 相机进行定位
- 计算出受试者位置形成发射和接收波束成型器
- WMC-VMD算法，分离受试者身体运动引起的弱生命体征信号



# 2 介绍

### 2.1 背景 
*Under the COVID-19 crisis, the demand for vital sign monitoring systems for tracking patients’ health conditions becomes more urgent*

在新冠疫情危机下，对跟踪患者健康状况的生命体征监测系统的需求变得更加紧迫


*Traditional vital sign measurements based on PPG and ECG sensors require wearing electrodes or chest bands, which are uncomfortable and inconvenient for some users like infants or burned patients.*
传感器的传统生命体征测量需要佩戴电极或胸带，这对于一些用户（如婴儿或烧伤患者）来说是不舒服和不方便的。



### 2.2所解决的问题

*For example, for indoor healthcare, the users may stay at a spot such as sofa, desk or table for a long time and perform activities such as computer work, exercise and entertainment, while other family members may walk past them in the room from time to time. Such complicated environment brings three challenges in contact-free vital sign monitoring, which have not been thoroughly addressed yet in the literature.*

例如，对于室内医疗保健，用户可能会在沙发、书桌或桌子等位置长时间停留，并进行诸如计算机工作、锻炼和娱乐等活动，而其他家庭成员可能会不时在房间内走动。如此复杂的环境给无接触生命体征监测带来了三个挑战，




#### 2.2.1 当前的问题
- 动态的环境中使用环境嘈杂可能产生多路径效应，出现人物的虚假对象，来自COTS毫米波雷达的距离单元数据可能会被错误地用于生命体征识别。



# 3 相关工作



# 4 动态环境中的信号模型
## 4.1 理想状态下的模型

1. 毫米波雷达的发送公式
$$
 x_T(t) = A_T e^{j2\pi \left(f_c t + \frac{B}{2T_c} t^2 + \phi_0 \right)} 。$$

这个是发送信号的公式。

这个公式中每个字段的含义如下：

- $$x_T(t) : 发送信号的时域表示。
- A_T : 发送信号的幅度。
-  e : 自然对数的底。
- j : 虚数单位。
- pi : 圆周率，约为3.14159。
-  f_c: 载波频率。
-  t : 时间。
-  B : 调制带宽。
-  T_c : 符号时间或信号的持续时间。
-  phi_0 : 初始相位。$$

总结起来，这个公式描述了一个经过相位调制的发送信号，其中信号的相位是时间的二次函数。



# 4.2 论文中在动态场景中的生命体征监测模型

论文中其他模型的内容
![[Pasted image 20240730173333.png]]
 图1 

如图1所示，上述模型在实际应用中效果不佳，原因在于静态多径效应、目标的身体运动以及动态环境（例如其他人经过）。在这种情况下，我们通过加入以下三个干扰因素来重建信号模型：静态物体、监测目标和其他移动的人。

---
静态物体：$$假设空间中有 (L) 个静态物体，第 (l) 个物体位于距离 (R_l) 和角度 (\theta_l) 处。由第 (k) 个天线接收到的总信号表示为：$$
$$ y_{\text{st}}(t, k) = \sum_{l=0}^{L-1} A_{l,k} e^{j2\pi \left(\alpha R_l t + \frac{2R_l}{\lambda} + \frac{k d \sin \theta_l}{\lambda} \right)},
$$
$其中 ( A_{l,k} ) 是第 ( l ) 个物体反射并由第 (k) 个天线接收到的信号幅度。$

监测目标：
包含目标的信号模型更加复杂。由于胸部运动和身体运动会导致小范围的变化，即使目标在同一位置，信号传播距离 $(R)$ 也会随时间变化。因此，我们将距离重新写为：
$$R(t) = R_{\text{tar}}(t) + \Delta R_{\text{tar}}(t), $$
其中 $( \Delta R_{\text{tar}}(t) )$ 表示由身体运动引起的小时间变化。这里$( R_{\text{tar}}(t)$ 表示在时间$(t)$ 的名义距离。对于在同一位置停留的监测目标，名义距离是恒定的。因此，第 $(k)$ 个天线接收到的目标信号可以表示为：
$$ y_{\text{tar}} (t, k) = A_{\text{tar}},k e^{j2\pi \left(\alpha R_{\text{tar}} t + \frac{2R_{\text{tar}} + 2\Delta R_{\text{tar}}(t)}{\lambda} + \frac{k d \sin \theta_{\text{tar}}}{\lambda} \right)}. $$


# 5 系统概述

本研究提出了一种融合系统 VaR-VSM，该系统使用摄像头和 MIMO 毫米波雷达实现动态生命体征监测。我们的方案参考了 COTS 毫米波设备 TI AWR1843 开发而成。该设备包含 3 个发射天线和 4 个接收天线。一个摄像头（Kinect V2）紧挨着雷达安装，并与雷达同步工作。尽管 Kinect 可以提供深度图像和 RGB 图像，但在我们的方案中仅使用 RGB 图像。当 Kinect 仅输出 RGB 图像时，它与传统的 RGB 摄像头没有区别。图 2 展示了我们系统的工作流程。每个主要处理模块的功能总结如下，后续将详细讨论。

![[Pasted image 20240730174447.png]]
- **摄像头模块**：捕捉目标的 RGB 图像信息，用于辅助雷达数据处理。
- **雷达模块**：使用 MIMO 毫米波雷达采集目标的生命体征信号，包含心跳和呼吸信息。
- **数据同步模块**：确保摄像头和雷达数据的同步，以便于后续的数据融合和处理。
- **信号处理模块**：对接收到的雷达信号进行处理，提取出有效的生命体征信号。
- **融合算法模块**：将摄像头的 RGB 图像信息与雷达的生命体征信号融合，提升监测的准确性和可靠性。
- **结果输出模块**：展示最终的监测结果，包括心率和呼吸率等关键生命体征指标。

通过以上各模块的协同工作，我们的 VaR-VSM 系统能够在动态环境中实现高效、准确的生命体征监测。


##  1 摄像头与毫米波雷达融合定位

COTS 毫米波雷达在快速时间维度上对原始 RadarCube 进行 1D 距离 FFT，这一步直接在雷达硬件层完成。我们使用 1D FFT 数据输出进行 AoA（到达角度）估计，并获得距离-角度热图。同时，摄像头捕捉图像流，并在图像上执行基于深度学习的目标检测框架。因此，我们融合距离-角度热图和检测到的人的轮廓以实现精确定位。详细内容将在第五节中描述。

## 2 基于位置的噪声抑制

一旦确定了每个目标的位置，就采用波束成形技术来提高目标反射信号的信噪比（SNR），并减少其他人的干扰。这包括发射波束成形（Tx-BF）和接收波束成形（Rx-BF）。Tx-BF 通过调整每个发射器的相位来实现。Tx-BF 形成的主波束可以指向目标。Rx-BF 确定一组权重，从而形成一个接收波束，也指向目标。这使得接收信号在该方向上获得最大功率。此部分将在第五节中详细介绍。

## 3 运动容错生命体征估计
为了减少运动干扰的影响，我们提出了一种通过优化连续范围单元上信号相位方差来重建生命体征信号的方法。此外，我们还提出了信号模式估计和功率谱熵（PSE）评估的方法，以自动测量信号的模式数量并加速计算。在此基础上，通过对重建的生命体征波形进行 FFT 计算心率和呼吸率。详细内容将在第六节中介绍。


# 6 摄像头与毫米波雷达融合定位

*雷达-摄像头融合在自动驾驶领域已经被广泛研究，用于跟踪或获得目标的高分辨率位置 [35], [36]。它主要利用摄像头的高角度分辨率和雷达的高深度分辨率来定位目标*受此启发，我们将摄像头引入雷达传感，通过在角度域利用目标检测框架来分离静止目标和移动干扰者。与这些工作主要关注融合雷达点和摄像头不同，本研究提出了融合距离-角度热图和摄像头的方法。即使两个人靠得很近，摄像头仍然可以通过较高的角度分辨率将其分开，而毫米波雷达由于天线数量有限，很难实现这一点。在摄像头的帮助下，雷达可以进一步进行噪声抑制和低干扰感应。然而，与现有的雷达-摄像头融合不同，我们面临以下新挑战：(1) 如何从所有检测到的物体中选择静止的人体目标，包括移动个体和静态家具；(2) 如何获得目标的深度（距离）。本节描述了一种通过融合图像中的目标轮廓和毫米波数据中的距离-角度热图来准确定位目标的方法。

## 6.1 从摄像头获取目标轮廓

摄像头捕捉的图像流由安装在雷达上的摄像头获取（如图3所示）。图像的分辨率为1920 × 1080像素，帧率设置为30帧每秒（FPS）。我们手动校准摄像头镜头的方向和高度，以使摄像头的视场（FOV）与毫米波雷达的视场对齐。一旦收集到图像，我们应用目标检测器YOLO V5 [37]来检测所有感兴趣的目标，每个目标在图像上都用矩形框框住。由于YOLO检测器能够分类多达80种类别，因此会存在许多包含非人体对象的冗余和不需要的边界框。因此，通过重新编程YOLO，我们自动选择仅包含人的边界框。此外，每个目标周围仍然存在许多相似的边界框。为了消除目标的模糊性，我们利用非极大值抑制（NMS）[37]算法来去除这些冗余框，获得最佳的一个。 



## 6.2 从毫米波雷达获取距离-角度热图

如图4所示，热图是通过将所有距离单元上的到达角（AoA）频谱叠加生成的。首先，雷达接收反射信号并进行I/Q解调。此步骤的输出称为中频（IF）信号。正如第二节所描述的，通过对所有通道的IF信号在快速时间维度上执行FFT，可以估计出距离频谱。在本文中，RadarCube被定义为此步骤的输出。空间FFT方法通常用于获取AoA频谱。

然而，在COTS毫米波雷达中，即使使用虚拟阵列技术，接收元件的数量也很少。例如，我们系统中使用的毫米波雷达在方位角方向上总共有8个虚拟接收天线。FFT方法无法提供足够高的AoA分辨率。尽管在FFT中经常使用零填充的方法，但仍存在严重的频谱泄漏，无法分离那些位置接近的对象。因此，我们引入了最小方差无失真响应（MVDR）[38]算法来获取细粒度的AoA频谱。

![[Pasted image 20240730175538.png]]


## 6.3定位静止目标

一旦获得距离-角度热图，就可以通过融合热图中的轮廓和峰值来估计监测目标的二维位置。首先，摄像头的角度视场（AFOV）范围是从 -60 度到 +60 度，由制造商设定。毫米波雷达的AFOV可以覆盖 -90 度到 +90 度的范围。然而，实验中使用的雷达发射器的主波束仅集中在约 ±60 度的范围内。其他方向的波束增益远低于主波束的增益，这使得雷达难以准确感知目标。在这种情况下，MVDR算法的AoA搜索范围设置为 ±60 度，从而实现摄像头与毫米波雷达的AFOV匹配。

在实际环境中，可能存在多个目标。我们利用摄像头数据过滤出用于生命体征识别的静止目标。请注意，这些目标的位置需要保持不变，而身体部位可能仍会有一些运动。基于图像的YOLO检测器的输出包括所有人的轮廓的起点坐标（x，y）、宽度w和高度h。当一个人移动时，其起点和宽度可能会随时间变化。对于目标，这些值几乎是恒定的。在这种情况下，我们可以设置两个阈值来删除移动目标的边界框。如果对应于一个人的边界框的起点（或宽度）的变化大于预定义的阈值，我们将其视为移动目标，从而忽略该边界框。

之后，我们可以将图像中静止目标的像素映射到热图上的角度单元中。通过在相应边界框的宽度内搜索最大值，可以获得每个目标的最佳AoA（如图5所示）。其表达式为：
\[ R_{\text{tar}}, \theta_{\text{tar}} = \arg\max_{R \in [0,10]} \max_{\theta \in \left[\frac{P_{\text{ang}}}{P_{\text{img}}}[x,x+w]\right]} H_m(R, \theta), \]
其中 \( H_m \) 表示通过MVDR估计的距离-角度热图（0到10代表热图上的相应范围值，\(\theta\) 是通过将图像像素重新映射到热图上获得的）。\(P_{\text{ang}}\) 和 \(P_{\text{img}}\) 分别表示距离-角度热图的角度单元数和图像的像素数。


``` PYTHON
import numpy as np
from scipy.signal import hilbert
from scipy.fftpack import fft, ifft

# 初始化
w = np.linalg.inv(s @ s.T) @ np.eye(s.shape[1]) / (np.eye(s.shape[1]).T @ np.linalg.inv(s @ s.T) @ np.eye(s.shape[1]))
sc = hilbert(s)  # 希尔伯特变换
S = fft(sc)  # FFT
Shat = choose_main_components(S)  # 选择主要成分

K = len(Shat)  # 奇异谱分析主成分数量
u_hat = np.zeros_like(S)  # 初始化模态函数
omega_hat = np.zeros(K)  # 初始化中心频率
lambda_hat = np.zeros(K)  # 初始化拉格朗日乘子

# 迭代过程
converged = False
n = 0

while not converged:
    n += 1
    for k in range(K):
        # 更新模态函数
        u_hat[k, :] = (np.sum(w * lambda_hat * Shat) - np.sum(u_hat[k, :])) / \
                      (1 + 2 * alpha * (omega - omega_hat[k])**2)
        
        # 更新中心频率
        omega_hat[k] = np.sum(omega * np.abs(u_hat[k, :])**2) / np.sum(np.abs(u_hat[k, :])**2)
    
    # 更新拉格朗日乘子
    for l in range(L):
        lambda_hat += eta * (Shat - np.sum(u_hat, axis=0))
    
    # 收敛性判断
    if np.linalg.norm(u_hat[:, n] - u_hat[:, n-1]) / np.linalg.norm(u_hat[:, n]) < epsilon:
        converged = True

# 零填充并进行逆FFT
u_t = np.fft.ifft(u_hat)

```